{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Second Lab - Week 1, Day 3\n",
    "\n",
    "Today we will work with lots of models! This is a way to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "from anthropic import AsyncAnthropic\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key not set (and this is optional)\n",
      "Google API Key exists and begins AI\n",
      "DeepSeek API Key not set (and this is optional)\n",
      "Groq API Key not set (and this is optional)\n"
     ]
    }
   ],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    print(f\"OpenAI API Key exists and begins {OPENAI_API_KEY[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if ANTHROPIC_API_KEY:\n",
    "    print(f\"Anthropic API Key exists and begins {ANTHROPIC_API_KEY[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if GOOGLE_API_KEY:\n",
    "    print(f\"Google API Key exists and begins {GOOGLE_API_KEY[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if DEEPSEEK_API_KEY:\n",
    "    print(f\"DeepSeek API Key exists and begins {DEEPSEEK_API_KEY[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if GROQ_API_KEY:\n",
    "    print(f\"Groq API Key exists and begins {GROQ_API_KEY[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation.'}]\n"
     ]
    }
   ],
   "source": [
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the ethical implications of developing highly autonomous artificial intelligence systems in terms of accountability, decision-making, and the potential impact on societal norms?\n"
     ]
    }
   ],
   "source": [
    "openai = AsyncOpenAI()\n",
    "response = await openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic model for storing LLM results\n",
    "class LLMResult(BaseModel):\n",
    "    model: str\n",
    "    answer: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "results: list[LLMResult] = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The API we know well\n",
    "async def openai_answer() -> None:\n",
    "\n",
    "  if OPENAI_API_KEY is None:\n",
    "    return None\n",
    "  \n",
    "  print(\"OpenAI starting!\")\n",
    "  model_name = \"gpt-4o-mini\"\n",
    "\n",
    "  try:\n",
    "    response = await openai.chat.completions.create(model=model_name, messages=messages)\n",
    "    answer = response.choices[0].message.content\n",
    "    results.append(LLMResult(model=model_name, answer=answer))\n",
    "  except Exception as e:\n",
    "    print(f\"Error with OpenAI: {e}\")\n",
    "    return None\n",
    "\n",
    "  print(\"OpenAI done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic has a slightly different API, and Max Tokens is required\n",
    "\n",
    "async def anthropic_answer() -> None:\n",
    "\n",
    "  if ANTHROPIC_API_KEY is None:\n",
    "    return None\n",
    "  \n",
    "  print(\"Anthropic starting!\")\n",
    "  model_name = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "  claude = AsyncAnthropic()\n",
    "  try:\n",
    "    response = await claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "    answer = response.content[0].text\n",
    "    results.append(LLMResult(model=model_name, answer=answer))\n",
    "  except Exception as e:\n",
    "    print(f\"Error with Anthropic: {e}\")\n",
    "    return None\n",
    "\n",
    "  print(\"Anthropic done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def google_answer() -> None:\n",
    "\n",
    "  if GOOGLE_API_KEY is None:\n",
    "    return None\n",
    "  \n",
    "  print(\"Google starting!\")\n",
    "  model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "  gemini = AsyncOpenAI(api_key=GOOGLE_API_KEY, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "  try:\n",
    "    response = await gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "    answer =  response.choices[0].message.content\n",
    "    results.append(LLMResult(model=model_name, answer=answer))\n",
    "  except Exception as e:\n",
    "    print(f\"Error with Google: {e}\")\n",
    "    return None\n",
    "\n",
    "  print(\"Google done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def deepseek_answer() -> None:\n",
    "\n",
    "  if DEEPSEEK_API_KEY is None:\n",
    "    return None\n",
    "  \n",
    "  print(\"DeepSeek starting!\")\n",
    "  model_name = \"deepseek-chat\"\n",
    "\n",
    "  deepseek = AsyncOpenAI(api_key=DEEPSEEK_API_KEY, base_url=\"https://api.deepseek.com/v1\")\n",
    "  try:\n",
    "    response = await deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "    answer = response.choices[0].message.content\n",
    "    results.append(LLMResult(model=model_name, answer=answer))\n",
    "  except Exception as e:\n",
    "    print(f\"Error with DeepSeek: {e}\")\n",
    "    return None\n",
    "\n",
    "  print(\"DeepSeek done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def groq_answer() -> None:\n",
    "\n",
    "  if GROQ_API_KEY is None:\n",
    "    return None\n",
    "  \n",
    "  print(\"Groq starting!\")\n",
    "  model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "  groq = AsyncOpenAI(api_key=GROQ_API_KEY, base_url=\"https://api.groq.com/openai/v1\")\n",
    "  try:\n",
    "    response = await groq.chat.completions.create(model=model_name, messages=messages)\n",
    "    answer = response.choices[0].message.content\n",
    "    results.append(LLMResult(model=model_name, answer=answer))\n",
    "  except Exception as e:\n",
    "    print(f\"Error with Groq: {e}\")\n",
    "    return None\n",
    "\n",
    "  print(\"Groq done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the next cell, we will use Ollama\n",
    "\n",
    "Ollama runs a local web service that gives an OpenAI compatible endpoint,  \n",
    "and runs models locally using high performance C++ code.\n",
    "\n",
    "If you don't have Ollama, install it here by visiting https://ollama.com then pressing Download and following the instructions.\n",
    "\n",
    "After it's installed, you should be able to visit here: http://localhost:11434 and see the message \"Ollama is running\"\n",
    "\n",
    "You might need to restart Cursor (and maybe reboot). Then open a Terminal (control+\\`) and run `ollama serve`\n",
    "\n",
    "Useful Ollama commands (run these in the terminal, or with an exclamation mark in this notebook):\n",
    "\n",
    "`ollama pull <model_name>` downloads a model locally  \n",
    "`ollama ls` lists all the models you've downloaded  \n",
    "`ollama rm <model_name>` deletes the specified model from your downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Super important - ignore me at your peril!</h2>\n",
    "            <span style=\"color:#ff7800;\">The model called <b>llama3.3</b> is FAR too large for home computers - it's not intended for personal computing and will consume all your resources! Stick with the nicely sized <b>llama3.2</b> or <b>llama3.2:1b</b> and if you want larger, try llama3.1 or smaller variants of Qwen, Gemma, Phi or DeepSeek. See the <A href=\"https://ollama.com/models\">the Ollama models page</a> for a full list of models and sizes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ollama_answer() -> None:\n",
    "  model_name = \"llama3.2\"\n",
    "\n",
    "  print(\"Ollama starting!\")\n",
    "  ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "  try:\n",
    "    response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "    answer = response.choices[0].message.content\n",
    "    results.append(LLMResult(model=model_name, answer=answer))\n",
    "  except Exception as e:\n",
    "    print(f\"Error with Ollama: {e}\")\n",
    "    return None\n",
    "\n",
    "  print(\"Ollama done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI starting!\n",
      "Google starting!\n",
      "Ollama starting!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama done!\n",
      "Google done!\n",
      "OpenAI done!\n"
     ]
    }
   ],
   "source": [
    "async def gather_answers():\n",
    "  tasks = [\n",
    "    openai_answer(),\n",
    "    anthropic_answer(),\n",
    "    google_answer(),\n",
    "    deepseek_answer(),\n",
    "    groq_answer(),\n",
    "    ollama_answer()\n",
    "  ]\n",
    "  await asyncio.gather(*tasks)\n",
    "\n",
    "await gather_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of competitors: 3\n",
      "# Response from competitor llama3.2\n",
      "\n",
      "The development of highly autonomous artificial intelligence (AI) systems raises several ethical implications related to accountability, decision-making, and societal norms. Some of the key concerns include:\n",
      "\n",
      "1. **Accountability**: As AI systems become more autonomous, it becomes increasingly challenging to assign responsibility for their actions. Who is accountable for an AI's decisions? The developer, the end-user, or the AI itself? This lack of clear accountability raises concerns about the potential for harm caused by faulty or biased AI systems.\n",
      "2. **Decision-making**: Autonomous AI systems will be making decisions without human oversight, which can lead to unknown and unintended consequences. Decision-making algorithms may prioritize efficiency or profit over human values, potentially leading to harmful outcomes.\n",
      "3. **Societal norms**: The introduction of autonomous AI systems can disrupt societal norms and values, particularly if they are not designed with consideration for these norms. For example, an AI system may be trained on biased data or optimize for a particular objective that conflicts with widely accepted social norms.\n",
      "4. **Transparency and explainability**: As AI systems become increasingly complex, it becomes more difficult to understand their decision-making processes. Lack of transparency and explainability can erode trust in AI systems and lead to difficulties in identifying and addressing errors or biases.\n",
      "5. **Value alignment**: The development of autonomous AI systems requires value alignment with human values and ethics. Ensuring that AI systems prioritize human well-being, dignity, and rights over other objectives is crucial for developing trustworthy and responsible AI systems.\n",
      "6. **Human agency and control**: Autonomous AI systems may limit human agency and control over decisions made by the system. This raises concerns about the loss of autonomy and control in critical areas, such as healthcare, transportation, or finance.\n",
      "7. **Bias and discrimination**: AI systems can perpetuate existing biases and discrimination if they are trained on biased data or designed with flawed objectives. This can exacerbate social inequalities and marginalization.\n",
      "\n",
      "To address these ethical implications, researchers and developers must prioritise:\n",
      "\n",
      "1. **Responsible AI design**: Developing AI systems that are transparent, explainable, and aligned with human values and ethics.\n",
      "2. **Accountability frameworks**: Establishing clear accountability mechanisms for AI-related decisions and actions.\n",
      "3. **Value-driven development**: Incorporating value-based considerations into AI system design and development processes.\n",
      "4. **Human oversight and control**: Ensuring that human professionals have oversight and control over AI decision-making processes, particularly in critical areas.\n",
      "5. **Regular testing and evaluation**: Conducting thorough testing and evaluation of AI systems to identify potential biases or negative consequences.\n",
      "\n",
      "By acknowledging these ethical implications and taking steps to address them, we can develop more responsible, trustworthy, and beneficial artificial intelligence systems that align with human values and promote a positive societal impact.\n",
      "\n",
      "# Response from competitor gemini-2.0-flash\n",
      "\n",
      "The development of highly autonomous artificial intelligence (AI) systems raises profound ethical implications across several key areas:\n",
      "\n",
      "**1. Accountability:**\n",
      "\n",
      "*   **The Blame Game:**  When an autonomous AI causes harm (e.g., self-driving car accident, biased loan denial), who is responsible?  Is it the programmer, the manufacturer, the owner, the AI itself (even though it lacks moral agency in the current understanding)?  Current legal frameworks often struggle to assign clear accountability.  This lack of accountability can erode public trust and hinder the adoption of AI.\n",
      "*   **Transparency and Explainability:** To hold someone accountable, we need to understand *why* the AI made a particular decision.  If the AI is a \"black box,\" offering no insight into its reasoning process, it becomes nearly impossible to identify the cause of the harm and assign blame.  Explainable AI (XAI) aims to address this, but even the most advanced XAI tools may not fully reveal the complex decision-making processes of deep learning models.\n",
      "*   **Legal Personality:**  The question of granting AI legal personality is a complex one.  While it could provide a framework for assigning responsibility, it also raises fundamental questions about rights, liabilities, and the very definition of personhood.  Giving AI rights could be seen as a slippery slope, while holding it liable without granting it due process raises fairness concerns.\n",
      "*   **Shifting Responsibility:** There's a risk that developers and companies might try to deflect responsibility by claiming the AI acted autonomously and was beyond their control.  This \"automation bias\" can lead to overlooking human oversight failures and a general erosion of accountability.\n",
      "\n",
      "**2. Decision-Making:**\n",
      "\n",
      "*   **Bias and Discrimination:** AI systems are trained on data, and if that data reflects existing societal biases (e.g., gender, race, socioeconomic status), the AI will likely perpetuate and even amplify those biases in its decision-making.  This can lead to unfair or discriminatory outcomes in areas like hiring, loan applications, criminal justice, and healthcare.\n",
      "*   **Algorithmic Fairness:** Defining and achieving \"fairness\" in AI is a challenging task.  There are multiple, often conflicting, definitions of fairness, and optimizing for one definition can inadvertently disadvantage other groups.  Careful consideration must be given to which fairness criteria are most appropriate for each application, and trade-offs must be explicitly acknowledged.\n",
      "*   **Value Alignment:** Ensuring that AI systems act in accordance with human values is crucial.  However, defining and encoding those values is a complex and subjective process.  Whose values should be encoded? How do we resolve conflicts between different values?  How do we prevent AI from optimizing for a narrow set of objectives at the expense of broader ethical considerations?\n",
      "*   **Contextual Awareness and Common Sense:**  While AI can excel at specific tasks, it often lacks the contextual awareness and common sense reasoning that humans possess.  This can lead to poor or even dangerous decisions in situations that require nuanced understanding of the world.  Over-reliance on AI without adequate human oversight can have serious consequences.\n",
      "*   **Erosion of Human Judgment:** As we increasingly rely on AI to make decisions, there's a risk that we will become less skilled at exercising our own judgment and critical thinking.  This could make us more vulnerable to manipulation and less able to question or challenge AI-driven decisions.\n",
      "\n",
      "**3. Societal Impact:**\n",
      "\n",
      "*   **Job Displacement:**  Autonomous AI systems have the potential to automate many jobs currently performed by humans, leading to widespread job displacement and economic inequality.  Addressing this requires proactive measures such as retraining programs, universal basic income, and adjustments to the social safety net.\n",
      "*   **Privacy and Surveillance:** AI-powered surveillance technologies can collect and analyze vast amounts of data about individuals, raising serious concerns about privacy, autonomy, and the potential for abuse.  Strong regulations and ethical guidelines are needed to limit the use of surveillance AI and protect individual rights.\n",
      "*   **Manipulation and Persuasion:** AI can be used to create highly personalized and persuasive content, potentially manipulating people's beliefs, attitudes, and behaviors.  This raises concerns about the integrity of democratic processes and the potential for social engineering on a massive scale.\n",
      "*   **Changes in Social Norms and Values:** Widespread adoption of autonomous AI could lead to significant shifts in social norms and values.  For example, increased reliance on AI companions could alter our relationships with each other.  It's important to consider the potential long-term impacts of AI on our social fabric and to proactively shape its development in a way that aligns with our desired future.\n",
      "*   **Weaponization:** The development of autonomous weapons systems (AWS) raises particularly grave ethical concerns.  The prospect of machines making life-or-death decisions without human intervention is deeply disturbing, and the potential for unintended consequences and escalation is significant.  Many advocate for a ban on the development and deployment of AWS.\n",
      "*   **Power Asymmetry:**  The benefits of AI are not distributed equally. Those who control the technology (corporations, governments) wield immense power.  This can exacerbate existing inequalities and lead to a concentration of wealth and influence.  Efforts should be made to ensure that the benefits of AI are shared more broadly and that its development is guided by democratic principles.\n",
      "\n",
      "**Addressing these ethical implications requires a multi-faceted approach:**\n",
      "\n",
      "*   **Developing ethical frameworks and guidelines:**  These frameworks should be developed through broad stakeholder engagement, incorporating perspectives from ethicists, policymakers, technologists, and the public.\n",
      "*   **Investing in research on AI ethics and safety:**  More research is needed to understand the potential risks and benefits of AI and to develop methods for mitigating those risks.\n",
      "*   **Promoting transparency and explainability in AI systems:**  Developers should prioritize creating AI systems that are transparent and understandable, allowing users to understand how decisions are made and to identify potential biases.\n",
      "*   **Developing robust accountability mechanisms:**  Legal and regulatory frameworks need to be updated to address the unique challenges posed by autonomous AI.\n",
      "*   **Educating the public about AI:**  It's important to raise public awareness about the potential impacts of AI, both positive and negative, and to empower people to make informed decisions about its use.\n",
      "*   **Fostering international collaboration:**  AI is a global technology, and its development and deployment should be guided by international cooperation.\n",
      "\n",
      "By proactively addressing these ethical implications, we can help ensure that AI is developed and used in a way that benefits humanity and promotes a more just and equitable future.  Failing to do so risks creating a future where AI amplifies existing inequalities, erodes individual autonomy, and undermines the foundations of a democratic society.\n",
      "\n",
      "\n",
      "# Response from competitor gpt-4o-mini\n",
      "\n",
      "The development of highly autonomous artificial intelligence (AI) systems raises significant ethical implications across several domains, particularly in terms of accountability, decision-making, and the potential impact on societal norms. Here are some key considerations:\n",
      "\n",
      "### 1. Accountability\n",
      "\n",
      "- **Responsibility Attribution**: Determining who is accountable for the actions of autonomous AI systems is complex. If an AI system makes a mistake—such as causing harm in an accident—who is responsible? Is it the developer, the user, or the AI itself? This ambiguity can lead to legal and ethical challenges.\n",
      "\n",
      "- **Transparency**: Many AI systems operate using complex algorithms and machine learning models that are not easily interpretable. This lack of transparency can make it difficult to hold parties accountable and to defend against misuses or failures.\n",
      "\n",
      "- **Trust in Systems**: With autonomous AI systems taking on more significant roles, there is a need to build trust in these systems. If accountability cannot be clearly established, it may lead to societal distrust in AI technologies.\n",
      "\n",
      "### 2. Decision-Making\n",
      "\n",
      "- **Bias and Fairness**: Autonomous systems may perpetuate or even exacerbate existing biases if they are trained on biased datasets. This raises ethical concerns around fairness, particularly in sensitive areas like hiring, law enforcement, and healthcare.\n",
      "\n",
      "- **Moral and Ethical Frameworks**: AI systems involved in decision-making (e.g., self-driving cars, healthcare diagnostics) must operate under ethical frameworks that align with human values. However, programming these values is challenging, and what is considered \"ethical\" can vary significantly across cultures and contexts.\n",
      "\n",
      "- **Human Oversight**: The level of human oversight required in AI decision-making remains a contentious issue. Autonomous systems can operate independently, but determining when and how much human intervention is necessary is critical for ethical governance.\n",
      "\n",
      "### 3. Impact on Societal Norms\n",
      "\n",
      "- **Shifts in Employment**: As AI systems become more capable, they may displace traditional jobs, raising concerns about unemployment and economic inequality. The societal norm around work, role distributions, and the nature of employment may shift dramatically.\n",
      "\n",
      "- **Privacy and Surveillance**: The proliferation of autonomous AI systems may lead to increased surveillance, potentially infringing on individual privacy rights. This can change societal views on privacy and increase the normalization of surveillance practices.\n",
      "\n",
      "- **Dependency on Technology**: The reliance on highly autonomous systems could lead to a shift in human agency and critical thinking. As people become more dependent on AI for decision-making, there may be a decline in skills that encourage independent thought and problem-solving.\n",
      "\n",
      "- **Redefining Ethics in Society**: The integration of autonomous AI into daily life may challenge existing ethical standards and cultural norms, prompting a reevaluation of concepts such as responsibility, moral reasoning, and the nature of trust.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "The ethical implications of developing highly autonomous AI systems are multifaceted and require ongoing discourse among technologists, ethicists, policymakers, and the public. It is imperative to establish robust frameworks that address accountability, ensure fairness in decision-making, and thoughtfully consider the societal changes that these technologies will impose. As these systems evolve, continuous reflection on their ethical use will be crucial to align technological advancements with societal values and norms.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "together = \"\"\n",
    "competitors = []\n",
    "answers = []\n",
    "\n",
    "for res in results:\n",
    "    competitor = res.model\n",
    "    answer = res.answer\n",
    "    competitors.append(competitor)\n",
    "    answers.append(answer)\n",
    "    together += f\"# Response from competitor {competitor}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\"\n",
    "\n",
    "print(f\"Number of competitors: {len(results)}\")\n",
    "print(together)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(results)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are judging a competition between 3 competitors.\n",
      "Each model has been given this question:\n",
      "\n",
      "What are the ethical implications of developing highly autonomous artificial intelligence systems in terms of accountability, decision-making, and the potential impact on societal norms?\n",
      "\n",
      "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
      "Respond with JSON, and only JSON, with the following format:\n",
      "{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}\n",
      "\n",
      "Here are the responses from each competitor:\n",
      "\n",
      "# Response from competitor llama3.2\n",
      "\n",
      "The development of highly autonomous artificial intelligence (AI) systems raises several ethical implications related to accountability, decision-making, and societal norms. Some of the key concerns include:\n",
      "\n",
      "1. **Accountability**: As AI systems become more autonomous, it becomes increasingly challenging to assign responsibility for their actions. Who is accountable for an AI's decisions? The developer, the end-user, or the AI itself? This lack of clear accountability raises concerns about the potential for harm caused by faulty or biased AI systems.\n",
      "2. **Decision-making**: Autonomous AI systems will be making decisions without human oversight, which can lead to unknown and unintended consequences. Decision-making algorithms may prioritize efficiency or profit over human values, potentially leading to harmful outcomes.\n",
      "3. **Societal norms**: The introduction of autonomous AI systems can disrupt societal norms and values, particularly if they are not designed with consideration for these norms. For example, an AI system may be trained on biased data or optimize for a particular objective that conflicts with widely accepted social norms.\n",
      "4. **Transparency and explainability**: As AI systems become increasingly complex, it becomes more difficult to understand their decision-making processes. Lack of transparency and explainability can erode trust in AI systems and lead to difficulties in identifying and addressing errors or biases.\n",
      "5. **Value alignment**: The development of autonomous AI systems requires value alignment with human values and ethics. Ensuring that AI systems prioritize human well-being, dignity, and rights over other objectives is crucial for developing trustworthy and responsible AI systems.\n",
      "6. **Human agency and control**: Autonomous AI systems may limit human agency and control over decisions made by the system. This raises concerns about the loss of autonomy and control in critical areas, such as healthcare, transportation, or finance.\n",
      "7. **Bias and discrimination**: AI systems can perpetuate existing biases and discrimination if they are trained on biased data or designed with flawed objectives. This can exacerbate social inequalities and marginalization.\n",
      "\n",
      "To address these ethical implications, researchers and developers must prioritise:\n",
      "\n",
      "1. **Responsible AI design**: Developing AI systems that are transparent, explainable, and aligned with human values and ethics.\n",
      "2. **Accountability frameworks**: Establishing clear accountability mechanisms for AI-related decisions and actions.\n",
      "3. **Value-driven development**: Incorporating value-based considerations into AI system design and development processes.\n",
      "4. **Human oversight and control**: Ensuring that human professionals have oversight and control over AI decision-making processes, particularly in critical areas.\n",
      "5. **Regular testing and evaluation**: Conducting thorough testing and evaluation of AI systems to identify potential biases or negative consequences.\n",
      "\n",
      "By acknowledging these ethical implications and taking steps to address them, we can develop more responsible, trustworthy, and beneficial artificial intelligence systems that align with human values and promote a positive societal impact.\n",
      "\n",
      "# Response from competitor gemini-2.0-flash\n",
      "\n",
      "The development of highly autonomous artificial intelligence (AI) systems raises profound ethical implications across several key areas:\n",
      "\n",
      "**1. Accountability:**\n",
      "\n",
      "*   **The Blame Game:**  When an autonomous AI causes harm (e.g., self-driving car accident, biased loan denial), who is responsible?  Is it the programmer, the manufacturer, the owner, the AI itself (even though it lacks moral agency in the current understanding)?  Current legal frameworks often struggle to assign clear accountability.  This lack of accountability can erode public trust and hinder the adoption of AI.\n",
      "*   **Transparency and Explainability:** To hold someone accountable, we need to understand *why* the AI made a particular decision.  If the AI is a \"black box,\" offering no insight into its reasoning process, it becomes nearly impossible to identify the cause of the harm and assign blame.  Explainable AI (XAI) aims to address this, but even the most advanced XAI tools may not fully reveal the complex decision-making processes of deep learning models.\n",
      "*   **Legal Personality:**  The question of granting AI legal personality is a complex one.  While it could provide a framework for assigning responsibility, it also raises fundamental questions about rights, liabilities, and the very definition of personhood.  Giving AI rights could be seen as a slippery slope, while holding it liable without granting it due process raises fairness concerns.\n",
      "*   **Shifting Responsibility:** There's a risk that developers and companies might try to deflect responsibility by claiming the AI acted autonomously and was beyond their control.  This \"automation bias\" can lead to overlooking human oversight failures and a general erosion of accountability.\n",
      "\n",
      "**2. Decision-Making:**\n",
      "\n",
      "*   **Bias and Discrimination:** AI systems are trained on data, and if that data reflects existing societal biases (e.g., gender, race, socioeconomic status), the AI will likely perpetuate and even amplify those biases in its decision-making.  This can lead to unfair or discriminatory outcomes in areas like hiring, loan applications, criminal justice, and healthcare.\n",
      "*   **Algorithmic Fairness:** Defining and achieving \"fairness\" in AI is a challenging task.  There are multiple, often conflicting, definitions of fairness, and optimizing for one definition can inadvertently disadvantage other groups.  Careful consideration must be given to which fairness criteria are most appropriate for each application, and trade-offs must be explicitly acknowledged.\n",
      "*   **Value Alignment:** Ensuring that AI systems act in accordance with human values is crucial.  However, defining and encoding those values is a complex and subjective process.  Whose values should be encoded? How do we resolve conflicts between different values?  How do we prevent AI from optimizing for a narrow set of objectives at the expense of broader ethical considerations?\n",
      "*   **Contextual Awareness and Common Sense:**  While AI can excel at specific tasks, it often lacks the contextual awareness and common sense reasoning that humans possess.  This can lead to poor or even dangerous decisions in situations that require nuanced understanding of the world.  Over-reliance on AI without adequate human oversight can have serious consequences.\n",
      "*   **Erosion of Human Judgment:** As we increasingly rely on AI to make decisions, there's a risk that we will become less skilled at exercising our own judgment and critical thinking.  This could make us more vulnerable to manipulation and less able to question or challenge AI-driven decisions.\n",
      "\n",
      "**3. Societal Impact:**\n",
      "\n",
      "*   **Job Displacement:**  Autonomous AI systems have the potential to automate many jobs currently performed by humans, leading to widespread job displacement and economic inequality.  Addressing this requires proactive measures such as retraining programs, universal basic income, and adjustments to the social safety net.\n",
      "*   **Privacy and Surveillance:** AI-powered surveillance technologies can collect and analyze vast amounts of data about individuals, raising serious concerns about privacy, autonomy, and the potential for abuse.  Strong regulations and ethical guidelines are needed to limit the use of surveillance AI and protect individual rights.\n",
      "*   **Manipulation and Persuasion:** AI can be used to create highly personalized and persuasive content, potentially manipulating people's beliefs, attitudes, and behaviors.  This raises concerns about the integrity of democratic processes and the potential for social engineering on a massive scale.\n",
      "*   **Changes in Social Norms and Values:** Widespread adoption of autonomous AI could lead to significant shifts in social norms and values.  For example, increased reliance on AI companions could alter our relationships with each other.  It's important to consider the potential long-term impacts of AI on our social fabric and to proactively shape its development in a way that aligns with our desired future.\n",
      "*   **Weaponization:** The development of autonomous weapons systems (AWS) raises particularly grave ethical concerns.  The prospect of machines making life-or-death decisions without human intervention is deeply disturbing, and the potential for unintended consequences and escalation is significant.  Many advocate for a ban on the development and deployment of AWS.\n",
      "*   **Power Asymmetry:**  The benefits of AI are not distributed equally. Those who control the technology (corporations, governments) wield immense power.  This can exacerbate existing inequalities and lead to a concentration of wealth and influence.  Efforts should be made to ensure that the benefits of AI are shared more broadly and that its development is guided by democratic principles.\n",
      "\n",
      "**Addressing these ethical implications requires a multi-faceted approach:**\n",
      "\n",
      "*   **Developing ethical frameworks and guidelines:**  These frameworks should be developed through broad stakeholder engagement, incorporating perspectives from ethicists, policymakers, technologists, and the public.\n",
      "*   **Investing in research on AI ethics and safety:**  More research is needed to understand the potential risks and benefits of AI and to develop methods for mitigating those risks.\n",
      "*   **Promoting transparency and explainability in AI systems:**  Developers should prioritize creating AI systems that are transparent and understandable, allowing users to understand how decisions are made and to identify potential biases.\n",
      "*   **Developing robust accountability mechanisms:**  Legal and regulatory frameworks need to be updated to address the unique challenges posed by autonomous AI.\n",
      "*   **Educating the public about AI:**  It's important to raise public awareness about the potential impacts of AI, both positive and negative, and to empower people to make informed decisions about its use.\n",
      "*   **Fostering international collaboration:**  AI is a global technology, and its development and deployment should be guided by international cooperation.\n",
      "\n",
      "By proactively addressing these ethical implications, we can help ensure that AI is developed and used in a way that benefits humanity and promotes a more just and equitable future.  Failing to do so risks creating a future where AI amplifies existing inequalities, erodes individual autonomy, and undermines the foundations of a democratic society.\n",
      "\n",
      "\n",
      "# Response from competitor gpt-4o-mini\n",
      "\n",
      "The development of highly autonomous artificial intelligence (AI) systems raises significant ethical implications across several domains, particularly in terms of accountability, decision-making, and the potential impact on societal norms. Here are some key considerations:\n",
      "\n",
      "### 1. Accountability\n",
      "\n",
      "- **Responsibility Attribution**: Determining who is accountable for the actions of autonomous AI systems is complex. If an AI system makes a mistake—such as causing harm in an accident—who is responsible? Is it the developer, the user, or the AI itself? This ambiguity can lead to legal and ethical challenges.\n",
      "\n",
      "- **Transparency**: Many AI systems operate using complex algorithms and machine learning models that are not easily interpretable. This lack of transparency can make it difficult to hold parties accountable and to defend against misuses or failures.\n",
      "\n",
      "- **Trust in Systems**: With autonomous AI systems taking on more significant roles, there is a need to build trust in these systems. If accountability cannot be clearly established, it may lead to societal distrust in AI technologies.\n",
      "\n",
      "### 2. Decision-Making\n",
      "\n",
      "- **Bias and Fairness**: Autonomous systems may perpetuate or even exacerbate existing biases if they are trained on biased datasets. This raises ethical concerns around fairness, particularly in sensitive areas like hiring, law enforcement, and healthcare.\n",
      "\n",
      "- **Moral and Ethical Frameworks**: AI systems involved in decision-making (e.g., self-driving cars, healthcare diagnostics) must operate under ethical frameworks that align with human values. However, programming these values is challenging, and what is considered \"ethical\" can vary significantly across cultures and contexts.\n",
      "\n",
      "- **Human Oversight**: The level of human oversight required in AI decision-making remains a contentious issue. Autonomous systems can operate independently, but determining when and how much human intervention is necessary is critical for ethical governance.\n",
      "\n",
      "### 3. Impact on Societal Norms\n",
      "\n",
      "- **Shifts in Employment**: As AI systems become more capable, they may displace traditional jobs, raising concerns about unemployment and economic inequality. The societal norm around work, role distributions, and the nature of employment may shift dramatically.\n",
      "\n",
      "- **Privacy and Surveillance**: The proliferation of autonomous AI systems may lead to increased surveillance, potentially infringing on individual privacy rights. This can change societal views on privacy and increase the normalization of surveillance practices.\n",
      "\n",
      "- **Dependency on Technology**: The reliance on highly autonomous systems could lead to a shift in human agency and critical thinking. As people become more dependent on AI for decision-making, there may be a decline in skills that encourage independent thought and problem-solving.\n",
      "\n",
      "- **Redefining Ethics in Society**: The integration of autonomous AI into daily life may challenge existing ethical standards and cultural norms, prompting a reevaluation of concepts such as responsibility, moral reasoning, and the nature of trust.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "The ethical implications of developing highly autonomous AI systems are multifaceted and require ongoing discourse among technologists, ethicists, policymakers, and the public. It is imperative to establish robust frameworks that address accountability, ensure fairness in decision-making, and thoughtfully consider the societal changes that these technologies will impose. As these systems evolve, continuous reflection on their ethical use will be crucial to align technological advancements with societal values and norms.\n",
      "\n",
      "\n",
      "\n",
      "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\n"
     ]
    }
   ],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"results\": [\"gemini-2.0-flash\", \"gpt-4o-mini\", \"llama3.2\"]}\n"
     ]
    }
   ],
   "source": [
    "# Judgement time!\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "judgement = response.choices[0].message.content\n",
    "print(judgement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: gemini-2.0-flash\n",
      "Rank 2: gpt-4o-mini\n",
      "Rank 3: llama3.2\n"
     ]
    }
   ],
   "source": [
    "# OK let's turn this into results!\n",
    "\n",
    "results_dict = json.loads(judgement)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, comp in enumerate(ranks):\n",
    "    print(f\"Rank {index+1}: {comp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
